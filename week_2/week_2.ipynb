{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Multivariable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "> $h_{\\theta}(\\mathbf{x}) = \\theta^{\\top}\\mathbf{x} = \\theta_{0}\\mathbf{x}_{0} + \\theta_{1}\\mathbf{x}_{1} + \\theta_{2}\\mathbf{x}_{2} + \\cdots + \\theta_{n}\\mathbf{x}_{n}$\n",
    ">\n",
    ">$x_{0} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "> $\\displaystyle J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(\\mathbf{x}^{(i)}) - y^{(i)})^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "repeat until convergence:\n",
    "\n",
    "> $\\displaystyle \\theta_{j} := \\theta_{j} - \\alpha\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta_{0},\\theta_{1},\\dots,\\theta{n}), j \\in \\{0,1,\\dots,n\\}$\n",
    ">\n",
    "> $\\alpha$ is the **learning rate**.\n",
    "\n",
    "or\n",
    "\n",
    "> $\\displaystyle \\theta_{j} := \\theta_{j} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) -y^{(i)})\\mathbf{x}_{j}^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "> get every feature into approximately a $-1 \\leqslant x_{i} \\leqslant +1$ range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Normalization\n",
    "\n",
    "make features have approximately zero mean and a $-0.5 \\leqslant x_{i} \\leqslant +0.5$ range.\n",
    "\n",
    "> $\\displaystyle x_{i} \\leftarrow \\frac{x_{i} - \\mu_{i}}{s_{i}}$\n",
    ">\n",
    "> where $\\mu_{i}$ is the average value of $x_{i}$ in the training set,\n",
    ">\n",
    "> and $s_{i}$ is the range (max - min) of the values of $x_{i}$ or his **standard deviation** when using the **standard score(z-score)**.\n",
    "\n",
    "We **never** normalize $x_0$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate\n",
    "\n",
    "plot the cost $J(\\theta)$ in regard to the number of iterations.\n",
    "\n",
    "- if $\\alpha$ is too small: slow convergence.\n",
    "- if $\\alpha$ is too large: $J(\\theta)$ may not decrease on every iteration; may not converge.\n",
    "\n",
    "To choose $\\alpha$, try: $0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "> $h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x + \\theta_{2}x^{2} + \\theta_{3}x^3$\n",
    "\n",
    "**Always** use feature scaling after adding polynomial features, but not on $x_{0}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "\n",
    "> $\\theta = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}, \\mathbf{X} \\in \\mathbb{R}^{m \\times n+1}$\n",
    ">\n",
    "> $m$ is the number of training examples.\n",
    ">\n",
    "> $n$ is the number of features.\n",
    ">\n",
    "> $\\displaystyle \\mathbf{X} = \\begin{bmatrix}\n",
    "    1 & \\mathbf{x}_{1}^{(1)} & \\mathbf{x}_{2}^{(1)} & \\cdots & \\mathbf{x}_{n}^{(1)} \\\\\n",
    "    1 & \\mathbf{x}_{1}^{(2)} & \\mathbf{x}_{2}^{(2)} & \\cdots & \\mathbf{x}_{n}^{(2)} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & \\mathbf{x}_{1}^{(m)} & \\mathbf{x}_{2}^{(m)} & \\cdots & \\mathbf{x}_{n}^{(m)}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{m \\times n+1}$\n",
    ">\n",
    "> $\\displaystyle \\mathbf{y} = \\begin{bmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    y^{(2)} \\\\\n",
    "    \\vdots \\\\\n",
    "    y^{(m)}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{m}$\n",
    ">    \n",
    "> $\\displaystyle \\theta = \\begin{bmatrix}\n",
    "    \\theta_{0} \\\\\n",
    "    \\theta_{1} \\\\\n",
    "    \\theta_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{n}\n",
    "    \\end{bmatrix} \\in \\mathbb{R}^{n+1}$\n",
    "    \n",
    "No need to use feature scaling.\n",
    "\n",
    "Use **gradient descent** over **normal equation** when we have more than 1000~10000 features.\n",
    "\n",
    "If $\\mathbf{X}^{\\top}\\mathbf{X}$ is non-invertible:\n",
    "\n",
    "- remove redundant features (linearly dependent).\n",
    "- delete features or use regularization if too many features ($m\\leqslant n$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
