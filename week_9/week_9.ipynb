{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "### Definition\n",
    "\n",
    "> Anomaly detection is the identification of rare observations which raise suspicions by **differing significantly from the majority of the data**.\n",
    "It is usually used for fraud detection, manufacturing defects or medical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use anomaly detection?\n",
    "\n",
    "- We build a model that defines $p(x)$ as the **probability** that $x$ is a normal data point (not anomalous).\n",
    "- We define a **probability threshold $\\mathcal{E}$** (usually **between 0 and 0.05**), depending on how sure we need to be.\n",
    "\n",
    "    - if $p(x_{test}) \\lt \\mathcal{E} \\rightarrow$ anomaly flag\n",
    "    - if $p(x_{test}) \\ge \\mathcal{E} \\rightarrow$ normal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian distribution (or Normal Distribution)\n",
    "\n",
    "For $x \\in \\mathbb{R}$, the **Gaussian equation** is:\n",
    "\n",
    "> $\\displaystyle p(x;\\mu,\\sigma^{2}) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\:exp\\Big({-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}\\Big)$\n",
    ">\n",
    "where $\\mu$ is the mean, $\\sigma$ is the standard deviation and $\\sigma^{2}$ is the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection algorithm\n",
    "\n",
    "For a training set $\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$, with each example $x \\in \\mathbb{R}^n$ and **normally distributed** $x_n \\sim \\mathcal{N}(\\mu_n, \\sigma_n^2)$\n",
    "\n",
    "1. Choose features $x_{i}$ that you think might be indicative of anomalous examples.\n",
    "2. Fit parameters $\\mu_{1}$, ..., $\\mu_{n}$, $\\sigma_{1}^{2}$, ..., $\\sigma_{n}^{2}$\n",
    "> $\\displaystyle\n",
    "\\begin{align}\n",
    "\\mu_{j} &= \\frac{1}{m} \\sum_{i=1}^{m}x_j^{(i)}\\\\\n",
    "\\sigma_{j}^{2} &= \\frac{1}{m} \\sum_{i=1}^{m}\\big(x_{j}^{(i)} - \\mu_{j}\\big)^{2}\n",
    "\\end{align}$\n",
    "\n",
    "3. Given new example $x$, compute $p(x)$:\n",
    ">$\\displaystyle p(x)=\n",
    "\\prod_{j=1}^{n}p(x_{j};\\mu_{j},\\sigma_{j}^{2})=\n",
    "\\prod_{j=1}^{n}\\underbrace{\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{j}}\\:exp\\Big({-\\frac{(x_{j}-\\mu_{j})^{2}}{2\\sigma_{j}^{2}}}\\Big)}_\\textbf{gaussian equation}$\n",
    "\n",
    "Anomaly if $p(x) \\lt \\mathcal{E}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm evaluation\n",
    "\n",
    "1. Set $\\mathcal{E}$ at some value.\n",
    "\n",
    "2. Fit model $p(x)$ on training set $\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$.\n",
    "\n",
    "3. On a cross validation/test set predict $x$:\n",
    ">$\n",
    "y = \\begin{cases}\n",
    "1 & \\text{if p(x)} \\lt \\mathcal{E} \\text{ (anomaly)} \\\\\n",
    "0 & \\text{if p(x)} \\ge \\mathcal{E} \\text{ (normal)} \\end{cases}$\n",
    "\n",
    "Possible evaluation metrics to use:\n",
    "- True positive, false positive, true negative, false negative\n",
    "- Precision/Recall ($F_{1}$-score):\n",
    "> $\\displaystyle F_{1} = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
    "\n",
    "Can also use cross-validation set to choose $\\mathcal{E}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection vs. Supervised learning\n",
    "\n",
    "**Anomaly detection:**\n",
    "- **Very small number of positive example** ($y = 1$) (0-20 is common).\n",
    "- Large number of negative examples ($y = 0$).\n",
    "- Many different \"types\" of anomalies. Hard for an algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen before.\n",
    "\n",
    "**Supervised learning:**\n",
    "- **Large number of positive and negative examples**.\n",
    "- Enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples are likely to be similar to ones in training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Features to use\n",
    "\n",
    "Plot a **histogram** of data to check it has a Gaussian description.\n",
    "\n",
    "If feature is **Non-Gaussian**, we can play with different **transformations** of the data to make it look more Gaussian:\n",
    "\n",
    "- **Log transformation**:\n",
    "    > $\\displaystyle x \\rightarrow log(x + c)$\n",
    "- **Exponent transformation**:\n",
    "    > $\\displaystyle x \\rightarrow x^{\\frac{1}{2}}, x^{\\frac{1}{3}}, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate gaussian distribution model\n",
    "\n",
    "$x \\in \\mathbb{R}^n$. Don't model $p(x_{(1)}), p(x_{(2)}), ...,$ etc separately. Model $p(x)$ all in one go. \n",
    "\n",
    "Parameters:\n",
    "> $\\mu \\in \\mathbb{R}^n$, $ \\Sigma \\in \\mathbb{R}^{n \\times n}$ (**covariance matrix**).\n",
    "\n",
    "The **Normal Gaussian** model is a **special case** of Multivariate Gaussian distribution where:\n",
    "\n",
    "> $\\displaystyle \\mathbf{\\Sigma} = \\begin{bmatrix}\n",
    "    \\sigma_{1}^{2} & 0 & \\cdots & 0 \\\\\n",
    "    0 & \\sigma_{2}^{2} & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & \\cdots & \\sigma_{n}^{2}\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Fit model $p(x)$ by setting:\n",
    "> $\\displaystyle\n",
    "\\begin{align}\n",
    "\\mu &= \\frac{1}{m} \\sum_{i=1}^{m}x^{(i)}\\\\\n",
    "\\Sigma &= \\frac{1}{m} \\sum_{i=1}^{m} (x^{(i)} - \\mu) (x^{(i)} - \\mu)^{\\top}\n",
    "\\end{align}$\n",
    "\n",
    "2. Given a new example $x$, compute:\n",
    "\n",
    "> $\\displaystyle p(x ; \\mu, \\Sigma) = \n",
    "\\frac{1}{(2\\pi) ^{\\frac{n}{2}}\\ \\lvert \\Sigma \\rvert ^{\\frac{1}{2}}} \\: exp\\left(-\\frac{1}{2} (x-\\mu)^{\\top} \\:\\Sigma^{-1} \\:(x-\\mu)\\right)$\n",
    "\n",
    "with $\\lvert \\Sigma \\rvert$ determinant of the **covariance matrix**\n",
    "\n",
    "Flag an anomaly if $p(x) \\lt \\mathcal{E}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original model vs. Multivariate model\n",
    "\n",
    "**Original model:**\n",
    "- $\\displaystyle p(x_{1};\\mu_{1},\\sigma_{1}^{2}) \\times \\cdots \\times p(x_{n};\\mu_{n},\\sigma_{n}^{2})$\n",
    "- Manually create features to capture anomalies where $x_1, x_2$ take unusual combinations of values\n",
    "- **Computationally cheaper** (alternatively, scales better to large $n$)\n",
    "- OK even if $m$ (training set size) is small\n",
    "\n",
    "**Multivariate Gaussian:**\n",
    "- $\\displaystyle p(x ; \\mu, \\Sigma) = \n",
    "\\frac{1}{(2\\pi) ^{\\frac{n}{2}}\\ \\lvert \\Sigma \\rvert ^{\\frac{1}{2}}} \\: exp\\Big(-\\frac{1}{2} (x-\\mu)^{\\top} \\:\\Sigma^{-1} \\:(x-\\mu)\\Big)$\n",
    "- Automatically captures correlations between features\n",
    "- **Computationally more expensive**\n",
    "- **Must have $m \\gt n$,** or else $\\Sigma$ is **non-invertible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender system\n",
    "\n",
    "Users have rated movies and we want to recommend them new ones.\n",
    "\n",
    "$r(i,j) = 1$ if user $j$ has rated movie $i$ (0 otherwise)\n",
    "\n",
    "$y^{(i,j)} = $ rating by user $j$ on movie $i$ (if defined)\n",
    "\n",
    "$\\theta ^{(j)} = $  parameter vector for user $j$\n",
    "\n",
    "$x^{(i)} = $ feature vector for movie $i$\n",
    "\n",
    "For user $j$, movie $i$, predicted rating: $(\\theta^{(j)})^{T}(x^{(i)})$ with $\\theta^{(j)} \\in \\mathbb{R}^{n+1}$\n",
    "\n",
    "$m^{(j)}$ = number of movies rated by user $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-based recommendations\n",
    "\n",
    "We know some features $x^{(i)}$ about the content we want to recommend. For example for movies, we know their category (action, comedy, romance...) and we want to recommend movies from their favorite category to users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "To learn $\\theta^{(j)}$ (parameter for user $j$):\n",
    "\n",
    ">$\\displaystyle \\min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) ^{2} + \\frac{\\lambda}{2} \\sum_{k=1}^{n}(\\theta_{k}^{(j)})^2$\n",
    "\n",
    "To learn $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n_{u})}$ (parameter for all users $n_{u}$):\n",
    "\n",
    ">$\\displaystyle \\min_{\\theta^{(1)}, ..., \\theta^{(n_{u})}} \\underbrace{ \\frac{1}{2} \\sum_{j=1}^{n_{u}} \\sum_{i:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) ^{2} + \\frac{\\lambda}{2} \\sum_{j=1}^{n_{u}} \\sum_{k=1}^{n}(\\theta_{k}^{(j)})^2}_{J(\\theta^{(1)},..., J(\\theta^{(n_{u})})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "> $\\displaystyle \\theta^{(j)}_{k} := \\theta^{(j)}_{k} - \\alpha \\sum_{i:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) x^{(i)}_{k} \\quad$     $\\text{(for k = 0)}$\n",
    "\n",
    "> $\\displaystyle \\theta^{(j)}_{k} := \\theta^{(j)}_{k} - \\alpha \\ \\Bigg( \\sum_{i:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) x^{(i)}_{k} + \\lambda \\theta^{(j)}_{k}\\Bigg)$     $\\text{(for k}$ $\\neq 0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "The features $x^{(i)}$ are unknown and we want to estimate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Given $\\theta^{(1)}, \\theta^{(2)}, ..., \\theta^{(n_{u})}$, to learn $x^{(i)}$:\n",
    "\n",
    ">$\\displaystyle \\min_{x^{(i)}} \\frac{1}{2} \\sum_{j:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) ^{2} + \\frac{\\lambda}{2} \\sum_{k=1}^{n}(x_{k}^{(i)})^2$\n",
    "\n",
    "Given $\\theta^{(1)}, ..., \\theta^{(n_{u})}$, to learn $x^{(1)}, ..., x^{(n_{m})}$:\n",
    "\n",
    ">$\\displaystyle \\min_{x^{(1)}, ..., x^{(n_{m})}} \\frac{1}{2} \\sum_{i=1}^{n_{m}} \\sum_{j:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) ^{2} + \\frac{\\lambda}{2} \\sum_{i=1}^{n_{m}} \\sum_{k=1}^{n}(x_{k}^{(i)})^2$\n",
    "\n",
    "\n",
    "To minimize simultaneously $\\theta^{(1)}, ..., \\theta^{(n_{u})}$ and $x^{(1)}, ..., x^{(n_{m})}$ :\n",
    "\n",
    ">$\\displaystyle J(x^{(1)}, ..., x^{(n_{m})}, \\theta^{(1)}, ..., \\theta^{(n_{u})}) = \\frac{1}{2} \\sum_{(i,j):r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) ^{2} + \\frac{\\lambda}{2} \\sum_{i=1}^{n_{m}} \\sum_{k=1}^{n}(x_{k}^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{j=1}^{n_{u}} \\sum_{k=1}^{n}(\\theta_{k}^{(j)})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "> $\\displaystyle x^{(i)}_{k} := x^{(i)}_{k} - \\alpha \\ \\Bigg( \\sum_{j:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) \\theta ^{(j)}_{k} + \\lambda x^{(i)}_{k}\\Bigg)$\n",
    "\n",
    "> $\\displaystyle \\theta^{(j)}_{k} := \\theta^{(j)}_{k} - \\alpha \\ \\Bigg( \\sum_{i:r(i,j)=1} \\big( (\\theta^{(j)})^{T} x^{(i)} - y^{(i,j)} \\big) x^{(i)}_{k} + \\lambda \\theta^{(j)}_{k}\\Bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering algorithm\n",
    "\n",
    "1. Initialize $x^{(1)}, ..., x^{(n_{m})}, \\theta^{(1)}, ..., \\theta^{(n_{u})}$ to small random values, **not zero**.\n",
    "\n",
    "2. Minimize $J(x^{(1)}, ..., x^{(n_{m})}, \\theta^{(1)}, ..., \\theta^{(n_{u})})$ using gradient descent.\n",
    "\n",
    "3. For a user with parameter $\\theta$ and a movie with learned features $x$ , predict a star rating of $\\theta^{T}x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find movies $j$ related to movie $i$ we need to calculate the smallest $\\| x^{(i)} - x^{(j)} \\|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementational detail mean normalization\n",
    "\n",
    "To avoid the algorithm to predict ratings of 0 (because the $\\theta$ is about 0) to a user that have not rated any movies, we need to perform mean normalization.\n",
    "\n",
    "1. Calculate the mean for each movie\n",
    "2. Substract the corresponding mean to each movie ratings (in order to have a mean of 0 for each movies)\n",
    "3. Use this new matrix to learn the parameters $\\theta^{(j)}$ (almost equal to 0) and $x^{(i)}$\n",
    "4. For user j, on movie i, we predict $(\\theta^{(j)})^{T} x^{(i)} + \\mu_{i}$ and obtain a prediction equal to the mean of the ratings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
