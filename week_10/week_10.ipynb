{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large scale machine learning\n",
    "\n",
    "Working with **large datasets** allow to get **low bias** and thus to achieve a **high performance** algorithm.\n",
    "\n",
    "During this whole chapter we will take the linear regression algorithm as an example but all also work for logistic regression and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem of batch gradient descent\n",
    "\n",
    "For example, we want to compute a batch gradient descent on a large dataset ($\\text{m}$ of several millions) with a linear regression. Here is a reminder of the formulas:\n",
    "\n",
    "> $\\displaystyle \n",
    "\\begin{align} \n",
    "h_{\\theta}(x) &= \\sum_{j=0}^{n}\\theta_j x_j \\\\\n",
    "J_{train}(\\theta) &= \\frac{1}{2m} \\sum_{i=1}^{m} \\big(h_{\\theta}(x^{(i)}) - y^{(i)} \\big)^2 \\\\\n",
    "\\theta_{j} &:= \\theta_{j} - \\alpha \\underbrace{\\frac{1}{m}\\sum_{i=1}^{m} \\big(h_{\\theta}(x^{(i)}) - y^{(i)} \\big)x_{j}^{(i)}}_{\\frac{\\partial}{\\partial \\theta_{j}} J_{\\theta}(train)}\n",
    "\\end{align}$\n",
    "\n",
    "The summation of the batch gradient descent needs to be done on all the $\\text m$ iterations resulting on a **computationnally expensive procedure**.\n",
    "\n",
    "Fortunately, several methods exist to optimize the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "The cost is computed on a **single example** $ (x^{(i)}, y^{(i)})$ at the time :\n",
    "\n",
    "> $\\displaystyle \n",
    "\\begin{align} \n",
    "cost \\big( \\theta, (x^{(i)}, y^{(i)}) \\big) &= \\frac{1}{2}\\big(h_{\\theta}(x^{(i)}) - y^{(i)} \\big)^2 \\\\\n",
    "J_{train}(\\theta) &= \\frac{1}{m} \\sum_{i=1}^{m} cost \\big( \\theta, (x^{(i)}, y^{(i)}) \\big)\n",
    "\\end{align}$\n",
    "\n",
    "1. Randomly shuffle the dataset\n",
    "2. Repeat for i = 1, ..., m and j = 0, ..., n :\n",
    "\n",
    "> $\\displaystyle \n",
    "\\theta_{j} := \\theta_{j} - \\alpha \\underbrace{\\big(h_{\\theta}(x^{(i)}) - y^{(i)} \\big)x_{j}^{(i)}}_{\\frac{\\partial}{\\partial \\theta_{j}}cost \\big( \\theta, (x^{(i)}, y^{(i)}) \\big)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "It uses $\\text{b}$ examples for each iteration. The mini-batch size $\\text{b}$ is usually comprised between 2 and 100.\n",
    "\n",
    "Say $\\text{b = 10}$ and $\\text{m = 1000}$, for $\\text{i = 1, 11, 21, ..., 991}$ :\n",
    "\n",
    "> $\\displaystyle \n",
    "\\theta_{j} := \\theta_{j} - \\alpha \\frac{1}{10} \\sum_{k=i}^{i+9} \\big(h_{\\theta}(x^{(k)}) - y^{(k)} \\big)x_{j}^{(k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent convergence\n",
    "\n",
    "- During learning, compute $cost \\big( \\theta, (x^{(i)}, y^{(i)}) \\big)$ before updating $\\theta$ using $(x^{(i)}, y^{(i)})$.\n",
    "\n",
    "- Every 1000 iterations (say), plot $cost \\big( \\theta, (x^{(i)}, y^{(i)}) \\big)$ averaged over the last 1000 examples processed by the algorithm.\n",
    "\n",
    "- Learning rate $\\alpha$ is typically held constant. We can slowly decrease $\\alpha$ over time if we want $\\theta$ to converge:\n",
    "> $\\displaystyle \\alpha = \\frac{const1}{IterationNumber + const2}$\n",
    "\n",
    "The constants 1 and 2 need to be set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map reduce\n",
    "\n",
    "For example, when splitting the dataset between 4 different machines, each machine will have to compute $\\big(h_{\\theta}(x^{(i)}) - y^{(i)} \\big)x_{j}^{(i)}$ on one fourth of the data.\n",
    "\n",
    "Then the four results are combined in the batch gradient descent formula.\n",
    "\n",
    "The splitting can also be done on several processing cores from a multi-core machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
