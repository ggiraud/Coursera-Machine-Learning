{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "Logistic Function (or Sigmoid Function).\n",
    "\n",
    "> $\\displaystyle h_{\\theta}(\\mathbf{x}) = g(\\theta^{\\top}\\mathbf{x})$\n",
    ">\n",
    "> $\\displaystyle g(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "or\n",
    "\n",
    "> $\\displaystyle h_{\\theta}(\\mathbf{x}) = \\frac{1}{1 + e^{-\\theta^{\\top}\\mathbf{x}}}$\n",
    ">\n",
    "> $0 \\leq h_{\\theta}(\\mathbf{x}) \\leq 1$\n",
    "\n",
    "$h_{\\theta}(\\mathbf{x})$ = estimated probability that $y = 1$ on input $\\mathbf{x}$.\n",
    "\n",
    "> $h_{\\theta}(\\mathbf{x}) = P(y = 1|\\mathbf{x};\\theta)$\n",
    ">\n",
    "> \"probability that $y = 1$, given $\\mathbf{x}$, parametrized by $\\theta$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary\n",
    "\n",
    "Predict \"$y = 1$\" if $h_{\\theta}(\\mathbf{x}) \\ge 0.5$\n",
    "\n",
    "> $g(\\theta^{\\top}\\mathbf{x}) \\ge 0.5$ when $\\theta^{\\top}\\mathbf{x} \\ge 0$ \n",
    "\n",
    "Predict \"$y = 0$\" if $h_{\\theta}(\\mathbf{x}) \\lt 0.5$\n",
    "\n",
    "> $g(\\theta^{\\top}\\mathbf{x}) \\lt 0.5$ when $\\theta^{\\top}\\mathbf{x} \\lt 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "The mean-square cost function used for linear regression is a **non-convex** function when used with sigmoid hypothesis !\n",
    "\n",
    "Instead we will use the **Cross-entropy** loss (or log loss).\n",
    "\n",
    "> $\\displaystyle Cost(h_{\\theta}(\\mathbf{x}), y) = \\left.\n",
    "    \\begin{cases}\n",
    "        -log(h_{\\theta}(\\mathbf{x})) & \\text{if } y = 1 \\\\\n",
    "        -log(1 - h_{\\theta}(\\mathbf{x})) & \\text{if } y = 0 \\\\\n",
    "    \\end{cases}\n",
    "    \\right\\}$\n",
    ">\n",
    "> $\\displaystyle Cost(h_{\\theta}(\\mathbf{x}), y) = -(y) log(h_{\\theta}(\\mathbf{x})) -(1 - y) log(1 - h_{\\theta}(\\mathbf{x}))$\n",
    "\n",
    "so\n",
    "\n",
    "> $\\displaystyle J(\\theta) = -\\frac{1}{m}\\bigg[\\sum_{i=1}^{m}y^{(i)} log(h_{\\theta}(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(\\mathbf{x}^{(i)}))\\bigg]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "> $\\displaystyle \\theta_{j} = \\theta_{j} - \\alpha\\frac{\\partial}{\\partial \\theta_{j}}J(\\theta_{0}, \\theta_{1},\\dots,\\theta_{n}), j \\in \\{0,1,\\dots,n\\}$\n",
    ">\n",
    "> $\\alpha$ is the **learning rate**.\n",
    "\n",
    "or\n",
    "\n",
    "> $\\displaystyle \\theta_{j} = \\theta_{j} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{j}^{(i)}$\n",
    "\n",
    "The algorithm looks identical to linear regression, but $h_{\\theta}(\\mathbf{x})$ is different !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimization\n",
    "\n",
    "Optimization Algorithms:\n",
    "\n",
    "- Gradient Descent\n",
    "- Conjugate Gradient\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "\n",
    "An example in Octave with $J(\\theta) = (\\theta_{1} - 5)^{2} + (\\theta_{2} - 5)^{2}$:\n",
    "\n",
    "```octave\n",
    "function [jVal, gradient] = costFunction(theta)\n",
    "    jVal = (theta(1)-5)^2 + (theta(2)-5)^2;\n",
    "    gradient = zeros(2,1);\n",
    "    gradient(1) = 2*(theta(1)-5);\n",
    "    gradient(2) = 2*(theta(2)-5);\n",
    "end\n",
    "```\n",
    "\n",
    "```octave\n",
    "options = optimset('GradObj','on','MaxIter',100);\n",
    "```\n",
    "\n",
    "```octave\n",
    "initialTheta = zeros(2,1);\n",
    "[optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    " \n",
    "Train a logistic regression classifier $h_{\\theta}^{(i)}(\\mathbf{x})$ for each class $i$ to predict the probability that $y = i$.\n",
    "\n",
    "On a new input $\\mathbf{x}$ , to make a prediction, pick the class $i$ that maximizes:\n",
    "\n",
    "> $\\displaystyle \\max_{i}h_{\\theta}^{(i)}(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem of Overfitting\n",
    "\n",
    "- An \"Underfit\" model is said to have \"High Bias\".\n",
    "- An \"Overfit\" model is said to have \"High Variance\".\n",
    "\n",
    "### Adressing Overfitting\n",
    "\n",
    "Options:\n",
    "\n",
    "1. Reduce number of features\n",
    "    - Manually select which features to keep.\n",
    "    - Model selection algorithm(later in course).\n",
    "2. Regularization\n",
    "    - Keep all the features, but reduce magnitude/values of parameters $\\theta_{j}$.\n",
    "    - Works well when we have  lot of features , each of which contributes a bit to predicting $y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Small values for $\\theta_{0},\\theta_{1},\\dots,\\theta_{n}$.\n",
    "\n",
    "- \"Simpler\" hypothesis\n",
    "- Less prone to overfitting\n",
    "\n",
    "> $\\displaystyle J(\\theta) = \\frac{1}{2m}\\left[\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})^{2} + \\lambda\\sum_{j=1}^{n}\\theta_{j}^{2}\\right]$\n",
    ">\n",
    "> $\\lambda$ is the **regularization parameter**.\n",
    ">\n",
    "> $j$ starts at $1$, we **do not** regularize $\\theta_{0}$!\n",
    "\n",
    "If $\\lambda$ is very large:\n",
    "\n",
    "> $h_{\\theta}(\\mathbf{x}) = \\theta_{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "> $\\displaystyle J(\\theta) = \\frac{1}{2m}\\left[\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})^{2}\\right] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "> $\\displaystyle \\theta_{0} := \\theta_{0} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{0}^{(i)}$, we **don't** penalize $\\theta_{0}$!\n",
    ">\n",
    "> $\\displaystyle \\theta_{j} := \\theta_{j} - \\alpha\\bigg[\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}\\bigg]$\n",
    "\n",
    "or:\n",
    "\n",
    "> $\\displaystyle \\theta_{j} := \\theta_{j}\\big(1 - \\alpha\\frac{\\lambda}{m}\\big) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{j}^{(i)}$\n",
    ">\n",
    "> $1 - \\alpha\\frac{\\lambda}{m}$ is usually $\\lt$ 1.\n",
    "\n",
    "### Normal Equation\n",
    "\n",
    "> $\\displaystyle \\mathbf{X} = \\begin{bmatrix}\n",
    "    (\\mathbf{x}^{(1)})^{\\top} \\\\\n",
    "    \\vdots \\\\\n",
    "    (\\mathbf{x}^{(m)})^{\\top}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m \\times (n+1)}$\n",
    ">\n",
    "> $\\displaystyle \\mathbf{y} = \\begin{bmatrix}\n",
    "    y^{(1)} \\\\\n",
    "    \\vdots \\\\\n",
    "    y^{(m)}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{m}$\n",
    ">\n",
    "> $\\displaystyle \\mathbf{\\theta} = \\Bigg(\\mathbf{X}^{\\top}\\mathbf{X} + \\lambda\\begin{bmatrix}\n",
    "    0 & 0 & 0 & \\cdots & 0 \\\\\n",
    "    0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "    0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}\\Bigg)^{-1}\\mathbf{X}^{\\top}\\mathbf{y} \\in \\mathbb{R}^{n+1}$\n",
    "\n",
    "If $m \\leq n$, $\\mathbf{X}^{\\top}\\mathbf{X}$ will be singular or \"non invertible\", using regularization will correct this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "> $\\displaystyle J(\\theta) = -\\frac{1}{m}\\bigg[\\sum_{i=1}^{m}y^{(i)} log(h_{\\theta}(\\mathbf{x}^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(\\mathbf{x}^{(i)}))\\bigg] + \\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_{j}^{2}$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "> $\\displaystyle \\theta_{0} := \\theta_{0} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{0}^{(i)}$, we **don't** penalize $\\theta_{0}$!\n",
    ">\n",
    "> $\\displaystyle \\theta_{j} := \\theta_{j} - \\alpha\\bigg[\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}\\bigg]$\n",
    "\n",
    "or:\n",
    "\n",
    "> $\\displaystyle \\theta_{j} := \\theta_{j}\\big(1 - \\alpha\\frac{\\lambda}{m}\\big) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(\\mathbf{x}^{(i)}) - y^{(i)})\\mathbf{x}_{j}^{(i)}$\n",
    ">\n",
    "> $1 - \\alpha\\frac{\\lambda}{m}$ is usually $\\lt$ 1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
