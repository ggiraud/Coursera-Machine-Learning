{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed, CSV, LinearAlgebra, Random, Distributions, OnlineStats\n",
    "using SparseArrays, SharedArrays, DistributedArrays\n",
    "using HDF5, Tables, DataFrames, SQLite, Suppressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/guillaume/Downloads/ml-latest/ratings.csv\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"ratings.csv\"\n",
    "bigfile = \"/Users/guillaume/Downloads/ml-latest/ratings.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CSV.File(\"ratings.csv\", rows=100836):\n",
       "Tables.Schema:\n",
       " :userId     Union{Missing, Int64}  \n",
       " :movieId    Union{Missing, Int64}  \n",
       " :rating     Union{Missing, Float64}\n",
       " :timestamp  Union{Missing, Int64}  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = CSV.File(file, use_mmap=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\n",
      "1,1,4.0,964982703\n",
      "1,3,4.0,964981247\n",
      "1,6,4.0,964982224\n",
      "1,47,5.0,964983815\n"
     ]
    }
   ],
   "source": [
    ";head -n 5 $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Rating\n",
    "    itemId::Int64\n",
    "    userId::Int64\n",
    "    value::Float64\n",
    "end\n",
    "\n",
    "mutable struct Item\n",
    "    weights::Vector{Float64}\n",
    "    bias::Float64\n",
    "end\n",
    "\n",
    "mutable struct User\n",
    "    weights::Vector{Float64}\n",
    "    bias::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open_h5_dataset (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_h5_file(filename::String)\n",
    "    h5open(filename, \"cw\") do fid\n",
    "    end\n",
    "end\n",
    "\n",
    "function initialize_h5_dataset(filename::String, dataset::String, dims::Tuple{Int,Int};\n",
    "        chunksize::Int64=1000)\n",
    "    m, n = dims\n",
    "    h5open(filename, \"r+\", \"libver_bounds\", (HDF5.H5F_LIBVER_LATEST, HDF5.H5F_LIBVER_LATEST)) do fid \n",
    "        d = d_create(fid, dataset, datatype(Float64), dataspace(m, n)) \n",
    "        for chunk in Iterators.partition(1:m, chunksize)\n",
    "            r = UnitRange(extrema(chunk)...)\n",
    "            l = length(r)\n",
    "            d[r,:] = hcat(zeros(l), rand(Normal(0.0, 1e-4), l, n-1))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function open_h5_dataset(fun::Function, filename::String, dataset::String)\n",
    "    fid = h5open(filename, \"r+\")\n",
    "    dset = fid[\"$(dataset)\"]\n",
    "    if ismmappable(dset)\n",
    "        dset = readmmap(dset)\n",
    "    end\n",
    "    try\n",
    "        fun(dset)\n",
    "    finally\n",
    "        close(fid)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csv2db (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function csv2db(f::Function, csv::CSV.File)\n",
    "    dbfile::String = tempname()\n",
    "    db::SQLite.DB = SQLite.DB(dbfile)\n",
    "    table::String = first(splitext(csv.name))\n",
    "    csv |> SQLite.load!(db, table)\n",
    "    try\n",
    "        f(db, table)\n",
    "    finally\n",
    "        rm(dbfile)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct UserFactorJob\n",
    "    id::Int64\n",
    "    input::Channel\n",
    "    output::Channel\n",
    "end\n",
    "\n",
    "mutable struct ItemFactorJob\n",
    "    id::Int64\n",
    "    input::Channel\n",
    "    output::Channel\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alsbiased2(db::SQLite.DB, table::String, k::Int64=10;\n",
    "        nepochs::Int64=10,\n",
    "        reg::Float64=0.0001,\n",
    "        cb::Union{Nothing, Function}=nothing)\n",
    "    \n",
    "    print(\"Computing metrics...\")\n",
    "    μ::Float64 = @suppress_err SQLite.query(db, \"SELECT AVG(rating) as mean FROM $table\")[1, :mean]\n",
    "    rated_items::Vector{Int64} = @suppress_err collect(skipmissing(SQLite.query(db, \"SELECT DISTINCT movieId FROM $table ORDER BY movieId\").movieId))\n",
    "    rating_users::Vector{Int64} = @suppress_err collect(skipmissing(SQLite.query(db, \"SELECT DISTINCT userId FROM $table ORDER BY userId\").userId))\n",
    "    m::Int64 = @suppress_err SQLite.query(db, \"SELECT MAX(movieId) as max_item FROM $table\")[1, :max_item]\n",
    "    n::Int64 = @suppress_err SQLite.query(db, \"SELECT MAX(userId) as max_user FROM $table\")[1, :max_user]\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    print(\"Sorting database...\")\n",
    "    @suppress_err SQLite.execute!(db, \"CREATE TABLE item_sorted AS SELECT movieId as itemId, userId, rating as value FROM $table ORDER BY itemId ASC\");\n",
    "    @suppress_err SQLite.execute!(db, \"CREATE TABLE user_sorted AS SELECT movieId as itemId, userId, rating as value FROM $table ORDER BY userId ASC\");\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    h5file::String = tempname()\n",
    "    println(h5file)\n",
    "    print(\"Creating HDF5 latent factors...\")\n",
    "    create_h5_file(h5file)\n",
    "    initialize_h5_dataset(h5file, \"P\", (m, k+1), chunksize=1000)\n",
    "    initialize_h5_dataset(h5file, \"Q\", (n, k+1), chunksize=100)\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    print(\"Creating jobs channel...\")\n",
    "    jobs = Channel{Channel}(0)\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    print(\"Creating results channel...\")\n",
    "    results = Channel{NamedTuple{(:id, :factor), Tuple{Int64, Vector{Float64}}}}(0)\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    @async open_h5_dataset(h5file, \"Q\") do Q\n",
    "        for r in results\n",
    "            Q[r.id,:] = r.factor\n",
    "            IJulia.clear_output(true)\n",
    "            println(\"user $(r.id) processed.\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @async open_h5_dataset(h5file, \"P\") do P\n",
    "        limit = 1000\n",
    "        offset = 0\n",
    "        current_user = nothing\n",
    "        user_chan = nothing\n",
    "        while true\n",
    "            df = @suppress_err SQLite.query(db, \"SELECT itemId, userId, value FROM user_sorted LIMIT $limit OFFSET $offset\")\n",
    "\n",
    "            for r in DataFrames.eachrow(df)\n",
    "                if r.userId != current_user\n",
    "                    current_user = r.userId\n",
    "                    \n",
    "                    if user_chan !== nothing\n",
    "                        close(user_chan)\n",
    "                    end\n",
    "                    \n",
    "                    user_chan = Channel{NamedTuple{(:rating, :factor),Tuple{Rating, Vector{Float64}}}}(0)\n",
    "                    put!(jobs, user_chan)\n",
    "                end\n",
    "                \n",
    "                data = (rating=Rating(r.itemId, r.userId, r.value - μ - P[r.itemId, 1]), factor=[1.0; P[r.itemId, 2:end]])\n",
    "                put!(user_chan, data)\n",
    "            end\n",
    "            \n",
    "            if size(df, 1) < limit\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            offset += limit\n",
    "        end\n",
    "        \n",
    "        if user_chan !== nothing\n",
    "            close(user_chan)\n",
    "        end\n",
    "        \n",
    "        close(jobs)\n",
    "    end\n",
    "\n",
    "   for chan in jobs\n",
    "        id = nothing\n",
    "        o = LinReg()\n",
    "        for data in chan\n",
    "            id = data.rating.userId\n",
    "            fit!(o, (data.factor, data.rating.value))\n",
    "        end\n",
    "        result = coef(o, reg)\n",
    "        put!(results, (id=id, factor=result))\n",
    "    end\n",
    "    \n",
    "    rm(h5file)\n",
    "end\n",
    "\n",
    "csv2db(csv) do db, table\n",
    "    alsbiased2(db, table, 100)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)\n",
    "\n",
    "function alsbiased3(db::SQLite.DB, table::String, k::Int64=10;\n",
    "        nepochs::Int64=10,\n",
    "        reg::Float64=0.0001,\n",
    "        cb::Union{Nothing, Function}=nothing)\n",
    "    \n",
    "    print(\"Computing metrics...\")\n",
    "    μ::Float64 = @suppress_err SQLite.query(db, \"SELECT AVG(rating) as mean FROM $table\")[1, :mean]\n",
    "    rated_items::Vector{Int64} = @suppress_err collect(skipmissing(SQLite.query(db, \"SELECT DISTINCT movieId FROM $table ORDER BY movieId\").movieId))\n",
    "    rating_users::Vector{Int64} = @suppress_err collect(skipmissing(SQLite.query(db, \"SELECT DISTINCT userId FROM $table ORDER BY userId\").userId))\n",
    "    m::Int64 = @suppress_err SQLite.query(db, \"SELECT MAX(movieId) as max_item FROM $table\")[1, :max_item]\n",
    "    n::Int64 = @suppress_err SQLite.query(db, \"SELECT MAX(userId) as max_user FROM $table\")[1, :max_user]\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    print(\"Sorting database...\")\n",
    "    @suppress_err SQLite.execute!(db, \"CREATE TABLE item_sorted AS SELECT movieId as itemId, userId, rating as value FROM $table ORDER BY itemId ASC\");\n",
    "    @suppress_err SQLite.execute!(db, \"CREATE TABLE user_sorted AS SELECT movieId as itemId, userId, rating as value FROM $table ORDER BY userId ASC\");\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    h5file::String = tempname()\n",
    "    println(h5file)\n",
    "    print(\"Creating HDF5 latent factors...\")\n",
    "    create_h5_file(h5file)\n",
    "    initialize_h5_dataset(h5file, \"P\", (m, k+1), chunksize=1000)\n",
    "    initialize_h5_dataset(h5file, \"Q\", (n, k+1), chunksize=100)\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    print(\"Creating jobs channel...\")\n",
    "    jobs = Channel{Channel}(0)\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    print(\"Creating results channel...\")\n",
    "    results = Channel{NamedTuple{(:id, :factor), Tuple{Int64, Vector{Float64}}}}(0)\n",
    "    println(\"DONE\")\n",
    "    \n",
    "    @async open_h5_dataset(h5file, \"Q\") do Q\n",
    "        for r in results\n",
    "            Q[r.id,:] = r.factor\n",
    "            IJulia.clear_output(true)\n",
    "            println(\"user $(r.id) processed.\")\n",
    "        end\n",
    "    end\n",
    "\n",
    "    @async open_h5_dataset(h5file, \"P\") do P\n",
    "        limit = 1000\n",
    "        offset = 0\n",
    "        current_user = nothing\n",
    "        user_chan = nothing\n",
    "        while true\n",
    "            df = @suppress_err SQLite.query(db, \"SELECT itemId, userId, value FROM user_sorted LIMIT $limit OFFSET $offset\")\n",
    "\n",
    "            for r in DataFrames.eachrow(df)\n",
    "                if r.userId != current_user\n",
    "                    current_user = r.userId\n",
    "                    \n",
    "                    if user_chan !== nothing\n",
    "                        close(user_chan)\n",
    "                    end\n",
    "                    \n",
    "                    user_chan = Channel{NamedTuple{(:rating, :factor),Tuple{Rating, Vector{Float64}}}}(0)\n",
    "                    put!(jobs, user_chan)\n",
    "                end\n",
    "                \n",
    "                data = (rating=Rating(r.itemId, r.userId, r.value - μ - P[r.itemId, 1]), factor=[1.0; P[r.itemId, 2:end]])\n",
    "                put!(user_chan, data)\n",
    "            end\n",
    "            \n",
    "            if size(df, 1) < limit\n",
    "                break\n",
    "            end\n",
    "            \n",
    "            offset += limit\n",
    "        end\n",
    "        \n",
    "        if user_chan !== nothing\n",
    "            close(user_chan)\n",
    "        end\n",
    "        \n",
    "        close(jobs)\n",
    "    end\n",
    "\n",
    "   for chan in jobs\n",
    "        id = nothing\n",
    "        o = LinReg()\n",
    "        for data in chan\n",
    "            id = data.rating.userId\n",
    "            fit!(o, (data.factor, data.rating.value))\n",
    "        end\n",
    "        result = coef(o, reg)\n",
    "        put!(results, (id=id, factor=result))\n",
    "    end\n",
    "    \n",
    "    rm(h5file)\n",
    "end\n",
    "\n",
    "csv2db(csv) do db, table\n",
    "    alsbiased2(db, table, 100)\n",
    "end\n",
    "\n",
    "rmprocs(nworkers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addprocs(4)\n",
    "\n",
    "println(workers())\n",
    "\n",
    "@everywhere using SparseArrays, SharedArrays, LinearAlgebra\n",
    "\n",
    "costs = []\n",
    "csv = CSV.File(file, use_mmap=true);\n",
    "ratings = (Rating(r.movieId, r.userId, r.rating) for r in csv);\n",
    "\n",
    "@time P, Q = alsbiased2(ratings, 100;\n",
    "            nepochs=10,\n",
    "            reg=0.001,\n",
    "            cb=(epoch, cost)->begin\n",
    "                println(\"epoch: $(epoch), cost: $(cost)\")\n",
    "                push!(costs, cost)\n",
    "            end)\n",
    "\n",
    "rmprocs(workers());"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
