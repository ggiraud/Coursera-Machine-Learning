{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a Learning Algorithm\n",
    "\n",
    "- Get more training examples\n",
    "- Try smaller set of features\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features\n",
    "- Try increasing $\\lambda$.\n",
    "- Try decreasing $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your Hypothesis\n",
    "\n",
    "Split your examples between a **Training Set (70%)** and a **Test Set (30%)**.\n",
    "\n",
    "If the data is not randomly ordered, it is better to randomly shuffle it before picking these sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing procedure for Linear Regression\n",
    "\n",
    "- Learn parameter $\\theta$ from training data (minimizing training error $J(\\theta)$).\n",
    "- Compute test set error:\n",
    "> $\\displaystyle J_{test}(\\theta) = \\frac{1}{2m_{test}}\\sum_{i=1}^{m_{test}}(h_{\\theta}(x_{test}^{(i)}) - y_{test}^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing procedure for Logistic Regression\n",
    "\n",
    "- Learn parameter $\\theta$ from training data.\n",
    "- Compute test set error:\n",
    "> $\\displaystyle J_{test}(\\theta) = -\\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}} y_{test}^{(i)}\\log(h_{\\theta}(x_{test}^{(i)})) + (1 -  y_{test}^{(i)})\\log(1 - h_{\\theta}(x_{test}^{(i)}))$\n",
    "- or the alternative **Misclassification error** (0/1 misclassification error):\n",
    "> $\\displaystyle err(h_{\\theta}(x),y) = \\begin{cases}\n",
    "    1 & \\text{ if } & h_{\\theta}(x) \\ge 0.5, y = 0 \\\\\n",
    "     & \\text{ or if } & h_{\\theta}(x) \\lt 0.5, y = 1 \\\\\n",
    "    0 & \\text{ otherwise} &\n",
    "\\end{cases}$\n",
    ">\n",
    "> $\\displaystyle \\text{Test error} = \\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}}err(h_{\\theta}(x_{test}^{(i)}),y_{test}^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training/Validation/Test sets\n",
    "\n",
    "To find the **degree $d$** of a polynomial feature:\n",
    "\n",
    "1. Split your examples between a **Training Set (60%)**, a **Cross Validation Set (20%)** and a **Test Set (20%)**.\n",
    "2. Fit $\\theta^{(d)}$ for different values of $d$ on the **Training Set**.\n",
    "> $\\displaystyle J_{train}(\\theta) = \\frac{1}{2m_{train}}\\sum_{i=1}^{m_{train}}(h_{\\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$\n",
    "3. Pick $\\theta^{(d)}$ with the lowest cost on **Validation Set**.\n",
    "> $\\displaystyle J_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_{i=1}^{m_{cv}}(h_{\\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$\n",
    "4. Estimate generalization error on **Test Set** with selected $\\theta^{(d)}$.\n",
    "> $\\displaystyle J_{test}(\\theta) = \\frac{1}{2m_{test}}\\sum_{i=1}^{m_{test}}(h_{\\theta}(x_{test}^{(i)}) - y_{test}^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Bias vs Variance in Model Selection\n",
    "\n",
    "If we have a **Bias (underfit)** problem:\n",
    "> $J_{train}(\\theta)$ and $J_{cv}(\\theta)$ will be high.\n",
    ">\n",
    "> $J_{cv}(\\theta) \\approx J_{train}(\\theta)$\n",
    "\n",
    "If we have a **Variance (overfit)** problem:\n",
    "> $J_{train}(\\theta)$ will be low.\n",
    ">\n",
    ">$J_{cv}(\\theta) \\gg J_{train}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Regularization Parameter $\\lambda$\n",
    "\n",
    "To find the **regularization parameter $\\lambda$** of a polynomial feature:\n",
    "\n",
    "1. Split your examples between a **Training Set (60%)**, a **Cross Validation Set (20%)** and a **Test Set (20%)**.\n",
    "2. Fit $\\theta^{(\\lambda)}$ for different values of $\\lambda$ on the **Training Set** using **regularized cost function**.\n",
    "> $\\displaystyle J_{train}(\\theta) = \\frac{1}{2m_{train}}\\sum_{i=1}^{m_{train}}(h_{\\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2 + \\frac{\\lambda}{2m_{train}}\\sum_{j=1}^{n}\\theta_{j}^{2}$\n",
    ">\n",
    "> $\\lambda \\in \\{0, 0.01, 0.02, 0.04, 0.08,\\dots, 10.24\\}$\n",
    "3. Pick $\\theta^{(\\lambda)}$ with the lowest cost on **Validation Set**, using **un-regularized cost function**.\n",
    "> $\\displaystyle J_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_{i=1}^{m_{cv}}(h_{\\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$\n",
    "4. Estimate generalization error on **Test Set** with selected $\\theta^{(\\lambda)}$, using **un-regularized cost function**.\n",
    "> $\\displaystyle J_{test}(\\theta) = \\frac{1}{2m_{test}}\\sum_{i=1}^{m_{test}}(h_{\\theta}(x_{test}^{(i)}) - y_{test}^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "To plot learning curves:\n",
    "\n",
    "1. Split your examples between a **Training Set (60%)**, a **Cross Validation Set (20%)** and a **Test Set (20%)**.\n",
    "2. Fit $\\theta^{(m)}$ for samples of size $m$ of the **Training Set** using **regularized or un-regularized cost function**.\n",
    "> $\\displaystyle J_{train}(\\theta) = \\frac{1}{2m_{train}}\\sum_{i=1}^{m_{train}}(h_{\\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$\n",
    ">\n",
    "> or\n",
    ">\n",
    "> $\\displaystyle J_{train}(\\theta) = \\frac{1}{2m_{train}}\\sum_{i=1}^{m_{train}}(h_{\\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2 + \\frac{\\lambda}{2m_{train}}\\sum_{j=1}^{n}\\theta_{j}^{2}$\n",
    "3. Plot $J(\\theta^{(m)})$ on **Training Set** for each $\\theta^{(m)}$, using **un-regularized cost function**.\n",
    "> $\\displaystyle J_{train}(\\theta) = \\frac{1}{2m_{train}}\\sum_{i=1}^{m_{train}}(h_{\\theta}(x_{train}^{(i)}) - y_{train}^{(i)})^2$\n",
    "4. Plot $J(\\theta^{(m)})$ on **Validation Set** for each $\\theta^{(m)}$, using **un-regularized cost function**.\n",
    "> $\\displaystyle J_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_{i=1}^{m_{cv}}(h_{\\theta}(x_{cv}^{(i)}) - y_{cv}^{(i)})^2$\n",
    "\n",
    "If a learning algorithm is suffering from **high bias**, getting more data will not help much.\n",
    "\n",
    "If a learning algorithm is suffering from **high variance**, getting more data is likely to help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding what to do next\n",
    "\n",
    "- Get more training examples $\\rightarrow$ **fixes high variance**.\n",
    "- Try smaller set of features $\\rightarrow$ **fixes high variance**.\n",
    "- Try getting additional features $\\rightarrow$ **fixes high bias**.\n",
    "- Try adding polynomial features $\\rightarrow$ **fixes high bias**.\n",
    "- Try increasing $\\lambda$ $\\rightarrow$ **fixes high variance**.\n",
    "- Try decreasing $\\lambda$ $\\rightarrow$ **fixes high bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Overfitting\n",
    "\n",
    "*Small* neural networks:\n",
    "\n",
    "- Are more prone to underfitting.\n",
    "- are computationally cheaper.\n",
    "\n",
    "*Large* neural networks:\n",
    "\n",
    "- Are more prone to overfitting, which can be adressed by using regularization ($\\lambda$).\n",
    "- Are computationally more expensive.\n",
    "- We can choose the number of hidden layers by fitting $\\theta^{(L)}$ for an increasing number of hidden layers($L$) on a **Training Set**,\n",
    "then pick the lowest cost on a **Validation Set**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Approach\n",
    "\n",
    "1. Start with a **simple algorithm** that you can implement quickly.\n",
    "Implement it and test it on your cross-validation data.\n",
    "2. Plot **learning curves** to decide if more data, or features, etc. are likely to help.\n",
    "3. **Error analysis**: Manually examine the examples(in cross-validation set) that your algorithm made errors on.\n",
    "See if you can spot any systematic trend in what type of example is made errors on by comparing model accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Metrics for Skewed Classes\n",
    "\n",
    "We have a case of **skewed classes** when we have far more examples of one class than the other classes.\n",
    "\n",
    "With **skewed classes** it becomes much harder to use classification accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall\n",
    "\n",
    "$y = 1$ in presence of a rare class that we want to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <td colspan=\"2\" rowspan=\"2\" align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "    <td colspan=\"2\" align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;Actual&nbsp;Class&nbsp;&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td rowspan=\"2\" align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
       "      Predicted<br />\n",
       "      &nbsp;&nbsp;Class&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;1&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;True&nbsp;&nbsp;<br />\n",
       "      positive\n",
       "    </td>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;False&nbsp;&nbsp;<br />\n",
       "      positive\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;0&nbsp;&nbsp;<br />\n",
       "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
       "    </td>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;False&nbsp;&nbsp;<br />\n",
       "      negative\n",
       "    </td>\n",
       "    <td align=\"left\" valign=\"top\">\n",
       "      &nbsp;&nbsp;True&nbsp;&nbsp;<br />\n",
       "      negative\n",
       "    </td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<table>\n",
    "  <tr>\n",
    "    <td colspan=\"2\" rowspan=\"2\" align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "    <td colspan=\"2\" align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;Actual&nbsp;Class&nbsp;&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\" align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
    "      Predicted<br />\n",
    "      &nbsp;&nbsp;Class&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;1&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;True&nbsp;&nbsp;<br />\n",
    "      positive\n",
    "    </td>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;False&nbsp;&nbsp;<br />\n",
    "      positive\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;0&nbsp;&nbsp;<br />\n",
    "      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "    </td>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;False&nbsp;&nbsp;<br />\n",
    "      negative\n",
    "    </td>\n",
    "    <td align=\"left\" valign=\"top\">\n",
    "      &nbsp;&nbsp;True&nbsp;&nbsp;<br />\n",
    "      negative\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**:\n",
    "\n",
    "> $\\displaystyle \\frac{\\text{True Positives}}{\\text{Predicted Positives}} = \n",
    "   \\frac{\\text{True Positives}}{\\text{True Positive}+\\text{False Positives}}$\n",
    ">\n",
    "> High precision is a good thing.\n",
    "   \n",
    "**Recall**:\n",
    "\n",
    "> $\\displaystyle \\frac{\\text{True Positives}}{\\text{Actual Positives}} = \n",
    "   \\frac{\\text{True Positives}}{\\text{True Positives}+\\text{False Negatives}}$\n",
    ">\n",
    "> High recall is a good thing.\n",
    "\n",
    "A classifier with **high presision** and **high recall** is a good classifier.\n",
    "\n",
    "**Precision/Recall** is often a better way to evaluate an algorithm in the presence of **skewed classes** than looking at classificator error or classificator accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F$_{1}$ Score (F Score)\n",
    "\n",
    "To compare precision/recall numbers:\n",
    "\n",
    "> $\\displaystyle 2\\frac{PR}{P+R}$\n",
    "\n",
    "- If $P = 0$ or $R = 0$ then F-Score $= 0$.\n",
    "- If $P = 1$ and $R = 1$ then F-Score $= 1$.\n",
    "- $0 \\le$ F-Score $\\le 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold\n",
    "\n",
    "In Logistic Regression:\n",
    "\n",
    "> $0 \\le h_{\\theta}(x) \\le 1$\n",
    ">\n",
    "> Predict $1$ if $h_{\\theta}(x) \\ge threshold$\n",
    ">\n",
    "> Predict $0$ if $h_{\\theta}(x) \\lt threshold$\n",
    "\n",
    "By varying the threshold we can control the trade-off between precision and recall.\n",
    "\n",
    "- With a high threshold, we get a high presision and a low recall.\n",
    "- With a low threshold, we get a low presision and a high recall.\n",
    "\n",
    "To find the optimal **threshold** for your model:\n",
    "- Evaluate different values of **threshold** on the **Cross Validation Set** and pick the one that gives you the maximum value of **F Score**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
