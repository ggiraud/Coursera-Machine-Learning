{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means\n",
    "\n",
    "K Means is a **Clustering Algorithm** that groups the data into coherent clusters.\n",
    "\n",
    "Inputs:\n",
    "- $K$ (number of clusters)\n",
    "- Training Set with **no labels** associated $\\{x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\}$, $x^{(i)} \\in \\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distortion Cost Function\n",
    "\n",
    "> $\\displaystyle J(c^{(i)},\\dots,c^{(m)},\\mu_1,\\dots,\\mu_K) = \\frac{1}{m}\\sum_{i=1}^{m}\\|x^{(i)}-\\mu_{c^{(i)}}\\|^2$\n",
    ">\n",
    "> J is **not convex**.\n",
    ">\n",
    "> $c^{(i)}$ = index of cluster $(1,2,\\dots,K)$ to wich example $x^{(i)}$ is currently assigned, $i \\in \\mathbb{R}^m$.\n",
    ">\n",
    "> $\\mu_k$ = cluster centroid $k$ $(k \\in \\{1,2,\\dots,K\\},\\:\\mu_k \\in \\mathbb{R}^n)$.\n",
    ">\n",
    "> $\\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "1. Randomly initialize $K$ **cluster centroids** $\\mu_1, \\mu_2, \\dots, \\mu_K$.\n",
    "2. Repeat:\n",
    "    -  for $i=1$ to $m$:\n",
    "        > Minimize $J$ w.r.t $\\:c^{(1)},\\dots,c^{(m)}$.\n",
    "        >\n",
    "        > $c^{(i)}$ = index (from $1$ to $K$) of cluster centroid **closest** to $x^{(i)}$.\n",
    "    -  for $k=1$ to $K$:\n",
    "        > Minimize $J$ w.r.t $\\:\\mu_{(1)},\\dots,\\mu_{(K)}$.\n",
    "        >\n",
    "        > $\\mu_k$ = **average (mean)** of points assigned to cluster $k$.\n",
    "\n",
    "If a **cluster has no points** assigned to it, just **eliminate** it, ended up with $K-1$ clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Initialization\n",
    "\n",
    "1. Randomly pick $K$ training examples, $k < m$.\n",
    "2. Set $\\mu_1, \\mu_2, \\dots, \\mu_K$ to these examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Optima\n",
    "\n",
    "To avoid K Means getting stuck at **local optima when $K < 10$** and increase the odds of finding the best possible clustering, we try **multiple random initialization**.\n",
    "\n",
    "For $i=1$ to $100$:\n",
    "- **Randomly initialize** K-means.\n",
    "- **Run** K-means. Get $c^{(1)},\\dots,c^{(m)},\\mu_{(1)},\\dots,\\mu^{(K)}$.\n",
    "- **Compute Distortion Cost Function** $J(c^{(1)},\\dots,c^{(m)},\\mu_{(1)},\\dots,\\mu^{(K)})$.\n",
    "\n",
    "**Pick** clustering that gave the **lowest cost**.\n",
    "\n",
    "When **$K$ is large** the optimal clustering is often found quite early and **this is not needed**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Number of Clusters ($K$)\n",
    "\n",
    "We can use the **elbow method** by plotting the **Cost** w.r.t the **number of clusters**. Although it's often not very useful.\n",
    "\n",
    "We can also choose **K** in regard of our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "**PCA** is a procedure that uses an orthogonal transformation to convert a set of examples of possibly correlated features into a set of values of **linearly uncorrelated variables** called **Principal Components**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "> $A \\in \\mathbb{R}^{m \\times n}$, $m$ is the number of examples, $n$ is the number of features.\n",
    "\n",
    "We need to choose **vectors** onto which to project the data $A$ as to **maximize the variance** of the data (or **minimize the projection error**).\n",
    "\n",
    "Before using PCA it is important to apply **mean normalization** and, if necessary (features are on different scales), **feature scaling** to the data $A$:\n",
    "\n",
    "> $\\displaystyle A_{normalized} = A - \\mu$\n",
    ">\n",
    "> or:\n",
    ">\n",
    "> $\\displaystyle A_{normalized} = \\frac{A - \\mu}{\\sigma}$\n",
    "> \n",
    "> $\\mu$ is the **vector of the features means**.\n",
    ">\n",
    "> $\\sigma$ is the **vector of the features standard-deviations**\n",
    "\n",
    "Compute the **Covariance Matrix** of the data $A$ (division by $m - 1$ is **optional**):\n",
    "\n",
    "> $\\displaystyle C = \\frac{A_{normalized}^{\\top} A_{normalized}}{m - 1}$\n",
    "\n",
    "Proceed to the **Singular Value Decomposition (SVD)** of the **Covariance Matrix $C$**:\n",
    "\n",
    "> $C = U \\Sigma V^{\\top}$\n",
    "\n",
    "where:\n",
    "\n",
    "> $C C^{\\top} = (U \\Sigma V^{\\top})(U \\Sigma V^{\\top})^{\\top} = U \\Sigma V^{\\top} V \\Sigma^{\\top} U^{\\top} = U \\Sigma^{2} U^{\\top}$\n",
    ">\n",
    "> $C^{\\top} C = (U \\Sigma V^{\\top})^{\\top}(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V \\Sigma^{2} V^{\\top}$\n",
    ">\n",
    "> $C = Q \\Lambda Q^{\\top}$\n",
    ">\n",
    "> $C = C^{\\top} \\longrightarrow U = V = Q$ and $\\Sigma = \\Lambda$\n",
    ">\n",
    "> $U$ and $V$ are **equal and othogonal** matrices which contains the **Principal Components**.\n",
    ">\n",
    "> $\\Sigma$ is a **diagonal** matrix which contains the **Singular Values**.\n",
    "\n",
    "so:\n",
    "> $U$ and $V$ are the **eigenvectors of $C$** (ordered according to $\\Sigma$) and $U=VÂ \\in \\mathbb{R}^{n \\times n}$.\n",
    ">\n",
    "> $\\Sigma$ are the **eigenvalues of $C$** (in decreasing order) and $\\Sigma \\in \\mathbb{R}^{n \\times n}$.\n",
    ">\n",
    "> $\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_{11} & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma_{22} & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & \\sigma_{33} & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots &\\sigma_{nn} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "We project the data $A$ on the **eigenvectors** of the subspace $V$ reduced to $n \\times k$ dimensions.\n",
    "\n",
    "> $A_{reduced} = A_{normalized} V_{reduced}; A_{normalized} \\in \\mathbb{R}^{m \\times n}, V_{reduced} \\in \\mathbb{R}^{n \\times k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the number of Principal Components\n",
    "\n",
    "To **choose $k$** (number of Principal Components), pick the smallest value of $k$ for which:\n",
    "\n",
    "> $\\displaystyle \\frac{\\sum_{i=1}^{k} \\sigma_{ii}}{\\sum_{i=1}^{m} \\sigma_{ii}} \\geq 0.99$ (99% of **variance retained**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction from compressed representation\n",
    "\n",
    "> $ A_{reconstructed} = A_{reduced}V_{reduced}^{\\top} + \\mu = A_{normalized}V_{reduced}V_{reduced}^{\\top} + \\mu$\n",
    ">\n",
    "> if $A$ is **standardized**:\n",
    ">\n",
    ">  $ A_{reconstructed} = (A_{normalized} \\times \\sigma) V_{reduced}V_{reduced}^{\\top} + \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "**Don't use PCA to prevent overfitting**, use **regularization instead**. This way it's less likely to throw away valuable information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
